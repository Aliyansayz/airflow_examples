# Airflow Server

* Open Airflow Server

Click on the below button to start apache airflow.

What is Apache Airflow.?

* Apache Airflow is an open-source platform used to programmatically author, schedule, and monitor workflows. It was originally created by Airbnb and is now maintained by the Apache Software Foundation.

Here are some basics of Apache Airflow:

*  DAGs (Directed Acyclic Graphs): Airflow uses DAGs to represent workflows.
*  A DAG is a collection of tasks with dependencies between them. The tasks in a DAG are executed in a specific order.

* Operators: Operators are the building blocks of tasks in Airflow. An operator is a Python class that defines a single task in a DAG. Airflow provides many built-in operators such as BashOperator, PythonOperator, and more.
* Sensors: Sensors are special types of operators that wait for some external event or condition to occur before they execute their task.

* Executors: Executors are responsible for executing tasks. Airflow supports several executors such as LocalExecutor, SequentialExecutor, and CeleryExecutor.

* Connections: Connections are used to store the information required to connect to external systems such as databases, APIs, and more.

* Variables: Variables are used to store key-value pairs that can be accessed from within a DAG.

* Web UI: Airflow provides a web-based user interface that allows you to view and manage your DAGs, tasks, and logs.

* CLI: Airflow provides a command-line interface (CLI) that allows you to interact with Airflow from the terminal.


### Overall, Airflow is a powerful tool for creating, scheduling, and monitoring workflows. It provides a flexible and scalable platform for building data pipelines and automating complex workflows.

After clicking the ‘Open Apache Airflow in IDE’ button, You will able to see the below screen
